{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction \nAutoencoder are special type of deep learning architecture that consist of two networks encoder and decoder.\nThe encoder, through a series of CNN and downsampling, learns a reduced dimensional representation of the input data while decoder  through the use of CNN and upsampling, attempts to regenerate the data from the these representations. A well-trained decoder is able to regenerated data that is identical or as close as possible to the original input data.\nAutoencoder are generally used for anamoly detection, denoising image, colorizing the images. Here, i am going to colorize the landscape images using autoencoder.","metadata":{}},{"cell_type":"markdown","source":"<img src = 'https://miro.medium.com/max/600/1*nqzWupxC60iAH2dYrFT78Q.png' >","metadata":{}},{"cell_type":"markdown","source":"## Image Colorization\nImage colorization using different softwares require large amount of human effort, time and skill.But special type of deep learning architecture called autoencoder has made this task quiet easy. Automatic image colorization often involves the use of a class of convolutional neural networks (CNN) called autoencoders. These neural networks are able to distill the salient features of an image, and then regenerate the image based on these learned features. ","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://tinyclouds.org/colorize/best/6.jpg\">","metadata":{}},{"cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport keras\nimport cv2\nfrom keras.layers import MaxPool2D,Conv2D,UpSampling2D,Input,Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import img_to_array\nimport os\nfrom tqdm import tqdm\nimport re\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-09-07T09:27:58.379023Z","iopub.execute_input":"2021-09-07T09:27:58.37937Z","iopub.status.idle":"2021-09-07T09:28:04.887317Z","shell.execute_reply.started":"2021-09-07T09:27:58.37934Z","shell.execute_reply":"2021-09-07T09:28:04.886337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting landscape image data,resizing them and appending in array\nTo get the image in sorted order i have defined the function sorted_alphanumeric. Here, I have used open cv library to read and resize images. Finally images are normalized and are converted to array and are appended in empty list","metadata":{}},{"cell_type":"code","source":"# to get the files in proper order\ndef sorted_alphanumeric(data):  \n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n    return sorted(data,key = alphanum_key)\n# defining the size of the image\nSIZE = 160\ncolor_img = []\npath = '../input/landscape-image-colorization/landscape Images/color'\nfiles = os.listdir(path)\nfiles = sorted_alphanumeric(files)\nfor i in tqdm(files):    \n    if i == '6000.jpg':\n        break\n    else:    \n        img = cv2.imread(path + '/'+i,1)\n        # open cv reads images in BGR format so we have to convert it to RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #resizing image\n        img = cv2.resize(img, (SIZE, SIZE))\n        img = img.astype('float32') / 255.0\n        color_img.append(img_to_array(img))\n\n\ngray_img = []\npath = '../input/landscape-image-colorization/landscape Images/gray'\nfiles = os.listdir(path)\nfiles = sorted_alphanumeric(files)\nfor i in tqdm(files):\n     if i == '6000.jpg':\n        break\n     else: \n        img = cv2.imread(path + '/'+i,1)\n\n        #resizing image\n        img = cv2.resize(img, (SIZE, SIZE))\n        img = img.astype('float32') / 255.0\n        gray_img.append(img_to_array(img))\n         \n   ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:28:04.889605Z","iopub.execute_input":"2021-09-07T09:28:04.890008Z","iopub.status.idle":"2021-09-07T09:29:03.246315Z","shell.execute_reply.started":"2021-09-07T09:28:04.88996Z","shell.execute_reply":"2021-09-07T09:29:03.245364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Color image and it's corresponding grayscale image","metadata":{}},{"cell_type":"code","source":"# defining function to plot images pair\ndef plot_images(color,grayscale):\n    plt.figure(figsize=(15,15))\n    plt.subplot(1,3,1)\n    plt.title('Color Image', color = 'green', fontsize = 20)\n    plt.imshow(color)\n    plt.subplot(1,3,2)\n    plt.title('Grayscale Image ', color = 'black', fontsize = 20)\n    plt.imshow(grayscale)\n   \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:29:03.250065Z","iopub.execute_input":"2021-09-07T09:29:03.250356Z","iopub.status.idle":"2021-09-07T09:29:03.259708Z","shell.execute_reply.started":"2021-09-07T09:29:03.250327Z","shell.execute_reply":"2021-09-07T09:29:03.258451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting image pair**","metadata":{}},{"cell_type":"code","source":"for i in range(3,10):\n     plot_images(color_img[i],gray_img[i])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:29:03.264148Z","iopub.execute_input":"2021-09-07T09:29:03.26455Z","iopub.status.idle":"2021-09-07T09:29:06.355137Z","shell.execute_reply.started":"2021-09-07T09:29:03.264465Z","shell.execute_reply":"2021-09-07T09:29:06.353179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Slicing and reshaping\nOut of 5000 images I have sliced them to two part. train images consist 4000 images  while test images contains 1000 images.\nAfter slicing the image array, I reshaped them so that images can be fed directly into our encoder network","metadata":{}},{"cell_type":"code","source":"train_gray_image = gray_img[:5500]\ntrain_color_image = color_img[:5500]\n\ntest_gray_image = gray_img[5500:]\ntest_color_image = color_img[5500:]\n# reshaping\ntrain_g = np.reshape(train_gray_image,(len(train_gray_image),SIZE,SIZE,3))\ntrain_c = np.reshape(train_color_image, (len(train_color_image),SIZE,SIZE,3))\nprint('Train color image shape:',train_c.shape)\n\n\ntest_gray_image = np.reshape(test_gray_image,(len(test_gray_image),SIZE,SIZE,3))\ntest_color_image = np.reshape(test_color_image, (len(test_color_image),SIZE,SIZE,3))\nprint('Test color image shape',test_color_image.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:29:06.359525Z","iopub.execute_input":"2021-09-07T09:29:06.36009Z","iopub.status.idle":"2021-09-07T09:29:07.876661Z","shell.execute_reply.started":"2021-09-07T09:29:06.360032Z","shell.execute_reply":"2021-09-07T09:29:07.875491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining our model\nEncoder layer of our model consist blocks of Convolution layer with different number of kernel and kernel_size. Here, Convolution is used for downsampling.\nSimilary, Decoder layer of our model consist of  transpose convolution layer with different kernel size. Here, Decoder layer upsample image downsampled by encoder.\nSince there is feature loss between the encoder and decoder layers so inorder to prevent feature loss i have concatenate corresponding encoder and decoder layers. Check U_Net architecture for better understanding......","metadata":{}},{"cell_type":"code","source":"from keras import layers\ndef down(filters , kernel_size, apply_batch_normalization = True):\n    downsample = tf.keras.models.Sequential()\n    downsample.add(layers.Conv2D(filters,kernel_size,padding = 'same', strides = 2))\n    if apply_batch_normalization:\n        downsample.add(layers.BatchNormalization())\n    downsample.add(keras.layers.LeakyReLU())\n    return downsample\n\n\ndef up(filters, kernel_size, dropout = False):\n    upsample = tf.keras.models.Sequential()\n    upsample.add(layers.Conv2DTranspose(filters, kernel_size,padding = 'same', strides = 2))\n    if dropout:\n        upsample.dropout(0.2)\n    upsample.add(keras.layers.LeakyReLU())\n    return upsample\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:29:07.879114Z","iopub.execute_input":"2021-09-07T09:29:07.879409Z","iopub.status.idle":"2021-09-07T09:29:07.892897Z","shell.execute_reply.started":"2021-09-07T09:29:07.879381Z","shell.execute_reply":"2021-09-07T09:29:07.891956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model():\n    inputs = layers.Input(shape= [160,160,3])\n    d1 = down(128,(3,3),False)(inputs)\n    d2 = down(128,(3,3),False)(d1)\n    d3 = down(256,(3,3),True)(d2)\n    d4 = down(512,(3,3),True)(d3)\n    \n    d5 = down(512,(3,3),True)(d4)\n    #upsampling\n    u1 = up(512,(3,3),False)(d5)\n    u1 = layers.concatenate([u1,d4])\n    u2 = up(256,(3,3),False)(u1)\n    u2 = layers.concatenate([u2,d3])\n    u3 = up(128,(3,3),False)(u2)\n    u3 = layers.concatenate([u3,d2])\n    u4 = up(128,(3,3),False)(u3)\n    u4 = layers.concatenate([u4,d1])\n    u5 = up(3,(3,3),False)(u4)\n    u5 = layers.concatenate([u5,inputs])\n    output = layers.Conv2D(3,(2,2),strides = 1, padding = 'same')(u5)\n    return tf.keras.Model(inputs=inputs, outputs=output)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:29:07.894629Z","iopub.execute_input":"2021-09-07T09:29:07.89545Z","iopub.status.idle":"2021-09-07T09:29:07.913051Z","shell.execute_reply.started":"2021-09-07T09:29:07.89537Z","shell.execute_reply":"2021-09-07T09:29:07.911427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:29:07.915268Z","iopub.execute_input":"2021-09-07T09:29:07.916172Z","iopub.status.idle":"2021-09-07T09:29:11.179772Z","shell.execute_reply.started":"2021-09-07T09:29:07.916118Z","shell.execute_reply":"2021-09-07T09:29:11.17884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting our model","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mean_absolute_error',\n              metrics = ['acc'])\n\nmodel.fit(train_g, train_c, epochs = 100,batch_size = 50,verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:29:11.184781Z","iopub.execute_input":"2021-09-07T09:29:11.185135Z","iopub.status.idle":"2021-09-07T09:55:12.356693Z","shell.execute_reply.started":"2021-09-07T09:29:11.185093Z","shell.execute_reply":"2021-09-07T09:55:12.355384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_gray_image,test_color_image)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:55:12.36035Z","iopub.execute_input":"2021-09-07T09:55:12.360751Z","iopub.status.idle":"2021-09-07T09:55:14.013267Z","shell.execute_reply.started":"2021-09-07T09:55:12.360718Z","shell.execute_reply":"2021-09-07T09:55:14.011755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# plotting colorized image along with grayscale and color image","metadata":{}},{"cell_type":"code","source":"# defining function to plot images pair\ndef plot_images(color,grayscale,predicted):\n    plt.figure(figsize=(15,15))\n    plt.subplot(1,3,1)\n    plt.title('Color Image', color = 'green', fontsize = 20)\n    plt.imshow(color)\n    plt.subplot(1,3,2)\n    plt.title('Grayscale Image ', color = 'black', fontsize = 20)\n    plt.imshow(grayscale)\n    plt.subplot(1,3,3)\n    plt.title('Predicted Image ', color = 'Red', fontsize = 20)\n    plt.imshow(predicted)\n   \n    plt.show()\n\nfor i in range(50,58):\n    predicted = np.clip(model.predict(test_gray_image[i].reshape(1,SIZE, SIZE,3)),0.0,1.0).reshape(SIZE, SIZE,3)\n    plot_images(test_color_image[i],test_gray_image[i],predicted)\n\n ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:55:14.015435Z","iopub.execute_input":"2021-09-07T09:55:14.016161Z","iopub.status.idle":"2021-09-07T09:55:20.174797Z","shell.execute_reply.started":"2021-09-07T09:55:14.016109Z","shell.execute_reply":"2021-09-07T09:55:20.173479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:55:20.176455Z","iopub.execute_input":"2021-09-07T09:55:20.177293Z","iopub.status.idle":"2021-09-07T09:55:20.318307Z","shell.execute_reply.started":"2021-09-07T09:55:20.177245Z","shell.execute_reply":"2021-09-07T09:55:20.317141Z"},"trusted":true},"execution_count":null,"outputs":[]}]}